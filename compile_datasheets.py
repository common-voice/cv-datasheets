#!/usr/bin/env python3
"""compile_datasheets.py.

Compiles templates, community data, and metadata from the cv-datasheets repo
into a single datasheets.json file for consumption by the bundler.

The bundler fetches this file from GitHub raw URL before job creation,
filters by modality, and dispatches per-locale payloads to each pod.

Usage:
    python compile_datasheets.py <version>

    Example:
        python compile_datasheets.py 24.0-2025-12-05

    Options:
        --output, -o    Output file path (default: releases/datasheets-v<version>.json)
        --pretty        Pretty-print JSON output (default: minified)
"""

import csv
import json
import re
import sys
from datetime import UTC, datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent))
from scripts.datasheet import CVDatasheet

# ---------------------------------------------------------------------------
# Type aliases (Python 3.12+)
# ---------------------------------------------------------------------------

type LangMap = dict[tuple[str, str], str]  # (modality, code) -> template_language
type FundingMap = dict[str, str]  # locale -> funder
type RequestMap = dict[tuple[str, str], str]  # (modality, code) -> issue_number
type NameInfo = dict[str, str]  # {"native_name": ..., "english_name": ...}
type NameMap = dict[tuple[str, str], NameInfo]  # (modality, code) -> name info
type FieldMapping = dict[str, str]  # section_title -> field_name

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

SCHEMA_VERSION = "1.0.0"

REPO_ROOT = Path(__file__).resolve().parent
TEMPLATES_PATH = REPO_ROOT / "templates"
CV_CORPUS_PATH = REPO_ROOT / "cv-corpus"
METADATA_PATH = REPO_ROOT / "metadata"

MODALITIES = ["scs", "sps"]

TEMPLATE_LANGS = {
    "scs": ["en", "es", "zh-TW"],
    "sps": ["en", "es"],
}

# Placeholders that are auto-generated by the bundler even when not
# explicitly marked @ AUTOMATICALLY GENERATED @ in the template.
# These are excluded from community_fields extraction.
AUTO_GENERATED_FIELDS = {
    "gender_table",
    "age_table",
    "sentences_sample",
    "questions_sample",
    "transcriptions_sample",
}


# ---------------------------------------------------------------------------
# Metadata readers
# ---------------------------------------------------------------------------


def read_tsv(path: Path) -> list[dict[str, str]]:
    """Read a TSV file with headers into a list of dicts."""
    with open(path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        return list(reader)


def load_metadata() -> tuple[LangMap, FundingMap, RequestMap, NameMap]:
    """
    Load all metadata files.

    Returns
    -------
    lang_map : (modality, code) -> template_language
    funding  : locale -> funder
    requests : (modality, code) -> issue_number
    names    : (modality, code) -> {native_name, english_name}
    """
    lang_map: LangMap = {}
    ds_langs_path = METADATA_PATH / "datasheet-languages.tsv"
    if ds_langs_path.exists():
        for row in read_tsv(ds_langs_path):
            lang_map[(row["modality"], row["code"])] = row["language_of_datasheet"]

    funding: FundingMap = {}
    funding_path = METADATA_PATH / "funding.tsv"
    if funding_path.exists():
        for row in read_tsv(funding_path):
            funding[row["locale"]] = row["funder"]

    requests: RequestMap = {}
    requests_path = METADATA_PATH / "language-requests.tsv"
    if requests_path.exists():
        for row in read_tsv(requests_path):
            modality = row.get("modality", "scs")
            requests[(modality, row["code"])] = row["issue"]

    # Language names — merge metadata.tsv with per-modality metadata.json
    names: NameMap = {}

    # Primary: metadata.tsv
    metadata_tsv_path = METADATA_PATH / "metadata.tsv"
    if metadata_tsv_path.exists():
        for row in read_tsv(metadata_tsv_path):
            key = (row["modality"], row["code"])
            native = row.get("native_name", "")
            english = row.get("english_name", "")
            names[key] = {
                "native_name": native if native != "_" else "",
                "english_name": english if english != "_" else "",
            }

    # Supplement: metadata/{modality}/metadata.json (fills gaps)
    for modality in MODALITIES:
        names_json_path = METADATA_PATH / modality / "metadata.json"
        if not names_json_path.exists():
            continue
        with open(names_json_path, encoding="utf-8") as f:
            data = json.loads(f.read(), strict=False)
        for code, info in data.items():
            key = (modality, code)
            if key not in names:
                names[key] = {
                    "native_name": info.get("native_name", ""),
                    "english_name": info.get("english_name", ""),
                }
            else:
                # Fill blanks from JSON
                if not names[key]["native_name"]:
                    names[key]["native_name"] = info.get("native_name", "")
                if not names[key]["english_name"]:
                    names[key]["english_name"] = info.get("english_name", "")

    return lang_map, funding, requests, names


# ---------------------------------------------------------------------------
# Template discovery
# ---------------------------------------------------------------------------


def discover_community_fields(template_text: str) -> FieldMapping:
    """
    Parse a template and discover community-editable fields.

    Scans each section for {{PLACEHOLDER}} markers in both content and
    HTML comments. Sections marked @ AUTOMATICALLY GENERATED @ and
    known auto-generated placeholders are excluded.

    Returns
    -------
    FieldMapping : section_title -> field_name (lowercased placeholder name)
    """
    ds = CVDatasheet(template_text)
    mapping: FieldMapping = {}

    for section in ds.sections:
        # Skip sections explicitly marked as auto-generated
        is_auto = any("AUTOMATICALLY GENERATED" in c for c in section.comments)
        if is_auto:
            continue

        # Look for {{PLACEHOLDER}} in content and comments
        searchable = section.content + " " + " ".join(section.comments)
        match = re.search(r"\{\{([A-Z_]+)\}\}", searchable)
        if not match:
            continue

        field_name = match.group(1).lower()

        # Skip known auto-generated fields (template may lack the marker)
        if field_name in AUTO_GENERATED_FIELDS:
            continue

        mapping[section.title] = field_name

    return mapping


# ---------------------------------------------------------------------------
# Community data extraction
# ---------------------------------------------------------------------------


def _clean_content(content: str) -> str:
    """
    Clean up extracted section content.

    Handles malformed HTML comments and SPS-style [Not provided] markers
    that CVDatasheet's regex doesn't fully strip (unclosed comments, etc.)
    """
    # Remove unclosed HTML comments at end of content (<!--... without -->)
    content = re.sub(r"<!--(?:(?!-->).)*$", "", content, flags=re.DOTALL).strip()

    # Remove trailing --> remnants from malformed comment blocks
    content = re.sub(r"^-->\s*", "", content).strip()
    content = re.sub(r"\s*-->$", "", content).strip()

    # Remove [Not provided] markers and surrounding remnants
    content = re.sub(
        r"\[Not provided\].*?(?:-->|$)", "", content, flags=re.DOTALL | re.IGNORECASE
    ).strip()

    return content


def extract_community_data(
    md_path: Path,
    field_mapping: FieldMapping,
) -> dict[str, str]:
    """
    Extract community-provided content from a rendered datasheet.

    For each section title in field_mapping, reads the section content.
    Returns empty string for sections that are missing, still contain
    unreplaced {{PLACEHOLDER}} markers, or have only boilerplate markers.
    """
    with open(md_path, encoding="utf-8") as f:
        text = f.read()

    ds = CVDatasheet(text)
    fields: dict[str, str] = {}

    for section_title, field_name in field_mapping.items():
        section = ds.get_section(section_title)
        if section is None:
            fields[field_name] = ""
            continue

        content = _clean_content(section.content)

        # Still has unreplaced placeholders
        if re.search(r"\{\{[A-Z_]+\}\}", content):
            fields[field_name] = ""
            continue

        # Empty after cleanup
        if not content:
            fields[field_name] = ""
            continue

        fields[field_name] = content

    return fields


# ---------------------------------------------------------------------------
# Release directory discovery
# ---------------------------------------------------------------------------


def find_release_dirs(modality: str, version: str) -> dict[str, Path]:
    """
    Find release directories for each template language.

    Handles two layouts:
        v23 style: cv-corpus/{modality}/{version}/final/{lang}/
        v24 style: cv-corpus/{modality}/{version}/{lang}/

    Returns dict mapping template_lang -> directory Path.
    """
    base = CV_CORPUS_PATH / modality / version
    if not base.exists():
        return {}

    result: dict[str, Path] = {}

    # Try v23 style first (has final/ subdirectory)
    final_dir = base / "final"
    if final_dir.exists():
        for lang_dir in sorted(final_dir.iterdir()):
            if lang_dir.is_dir():
                result[lang_dir.name] = lang_dir
        if result:
            return result

    # Fall back to v24 flat style
    for entry in sorted(base.iterdir()):
        if entry.is_dir() and entry.name not in ("draft", "final"):
            result[entry.name] = entry

    return result


# ---------------------------------------------------------------------------
# Main compilation
# ---------------------------------------------------------------------------


def compile_datasheets(
    version: str, output_path: Path, *, pretty: bool = False
) -> None:
    """Compile all templates, community data, and metadata into a single JSON file."""
    print(f"Compiling datasheets for version {version}...")
    print(f"Output: {output_path}")

    # Load metadata
    lang_map, funding, requests, names = load_metadata()

    output = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": datetime.now(UTC).isoformat(),
        "source_version": version,
        "templates": {},
        "locales": {},
    }

    for modality in MODALITIES:
        print(f"\n{'='*60}")
        print(f"  {modality.upper()}")
        print(f"{'='*60}")

        # --- Templates ---
        output["templates"][modality] = {}
        template_field_maps: dict[str, FieldMapping] = {}

        for lang in TEMPLATE_LANGS.get(modality, []):
            tpl_path = TEMPLATES_PATH / modality / f"{lang}.md"
            if not tpl_path.exists():
                print(f"  [WARN] Template not found: {tpl_path}")
                continue

            with open(tpl_path, encoding="utf-8") as f:
                template_text = f.read()

            output["templates"][modality][lang] = template_text
            field_map = discover_community_fields(template_text)
            template_field_maps[lang] = field_map
            field_names = list(field_map.values())
            print(f"  Template '{lang}': {len(field_map)} community fields")
            print(f"    Fields: {field_names}")

        # --- Release directories ---
        release_dirs = find_release_dirs(modality, version)
        if release_dirs:
            print(f"  Release dirs: {list(release_dirs.keys())}")
        else:
            print(f"  [WARN] No release directory for {modality}/{version}")

        # --- Collect all locales ---
        modality_locales: set[str] = set()

        # From datasheet-languages.tsv
        for mod, code in lang_map:
            if mod == modality:
                modality_locales.add(code)

        # From release directory files
        for dir_path in release_dirs.values():
            for md_file in dir_path.glob("*.md"):
                if md_file.name.lower() == "readme.md":
                    continue
                modality_locales.add(md_file.stem)

        print(f"  Total locales: {len(modality_locales)}")

        # --- Process each locale ---
        output["locales"][modality] = {}
        filled_count = 0

        for locale in sorted(modality_locales):
            template_lang = lang_map.get((modality, locale), "en")

            # Metadata for this locale
            name_info = names.get((modality, locale), {})
            locale_metadata = {
                "native_name": name_info.get("native_name", ""),
                "english_name": name_info.get("english_name", ""),
                "funding": funding.get(locale, ""),
                "language_request_issue": requests.get((modality, locale), ""),
            }

            # Community fields — default all empty
            field_mapping = template_field_maps.get(template_lang, {})
            community_fields = dict.fromkeys(field_mapping.values(), "")

            # Try to extract from rendered datasheet
            if template_lang in release_dirs:
                md_path = release_dirs[template_lang] / f"{locale}.md"
                if md_path.exists():
                    extracted = extract_community_data(md_path, field_mapping)
                    community_fields.update(extracted)

            has_community = any(v for v in community_fields.values())
            if has_community:
                filled_count += 1

            output["locales"][modality][locale] = {
                "template_language": template_lang,
                "metadata": locale_metadata,
                "community_fields": community_fields,
            }

        total = len(output["locales"][modality])
        print(f"  Compiled: {total} locales ({filled_count} with community data)")

    # --- Write output ---
    indent = 2 if pretty else None
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=indent, ensure_ascii=False)

    file_size = output_path.stat().st_size
    size_str = (
        f"{file_size / (1024*1024):.1f} MB"
        if file_size > 1024 * 1024
        else f"{file_size / 1024:.1f} KB"
    )
    print(f"\nDone. Output: {output_path} ({size_str})")

    # Summary
    for modality in MODALITIES:
        tpls = len(output["templates"].get(modality, {}))
        locs = len(output["locales"].get(modality, {}))
        print(f"  {modality.upper()}: {tpls} templates, {locs} locales")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def main() -> None:
    """CLI entry point."""
    args = sys.argv[1:]

    if not args or args[0] in ("-h", "--help"):
        print((__doc__ or "").strip())
        sys.exit(0)

    version = args[0]
    releases_dir = REPO_ROOT / "releases"
    releases_dir.mkdir(exist_ok=True)
    output_path = releases_dir / f"datasheets-v{version}.json"
    pretty = False

    i = 1
    while i < len(args):
        if args[i] in ("--output", "-o") and i + 1 < len(args):
            output_path = Path(args[i + 1])
            i += 2
        elif args[i] == "--pretty":
            pretty = True
            i += 1
        else:
            print(f"Unknown option: {args[i]}", file=sys.stderr)
            sys.exit(1)

    compile_datasheets(version, output_path, pretty=pretty)


if __name__ == "__main__":
    main()
